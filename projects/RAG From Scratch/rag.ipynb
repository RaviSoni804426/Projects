{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc4e531",
   "metadata": {},
   "source": [
    "Build Your First RAG System From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41252aa1",
   "metadata": {},
   "source": [
    "Forget expensive APIs and proprietary databases. We’re doing this with the tools that real engineers use to build powerful, scalable systems. Here are the tools we will be using:\n",
    "\n",
    "1. transformers (Hugging Face): To get our powerful, free LLM.\n",
    "\n",
    "2. sentence-transformers: The easiest way to get a top-tier embedding model.\n",
    "\n",
    "3. faiss-cpu: Facebook AI’s blazing-fast, free vector search library. It’s our vector store.\n",
    "\n",
    "4. langchain: We’ll only use its text splitter, which is a smart shortcut that saves us hours of regex pain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc4387",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a101c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5 chunks:\n",
      "--- Chunk 1 ---\n",
      "Company Policy Manual:\n",
      "\n",
      "--- Chunk 2 ---\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Thursdays. Mondays and Fridays are optional remote days.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "- PTO Policy: Full-time employees receive 20 days of Paid Time Off (PTO) per year. PTO accrues monthly.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "- Tech Stack: The official backend language is Python, and the official frontend framework is React. For mobile development, we use React Native.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load our document\n",
    "with open(\"my_knowledge.txt\") as f:\n",
    "    knowledge_text = f.read()\n",
    "\n",
    "# 1. Initialize the Text Splitter\n",
    "# This splitter is smart. It tries to split on paragraphs (\"\\n\\n\"),\n",
    "# then newlines (\"\\n\"), then spaces (\" \"), to keep semantically\n",
    "# related text together as much as possible.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,  # Max size of a chunk\n",
    "    chunk_overlap=20, # Overlap to maintain context between chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# 2. Create the chunks\n",
    "chunks = text_splitter.split_text(knowledge_text)\n",
    "\n",
    "print(f\"We have {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8627d",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87970b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60060b861da643fa9e9b62d49fd55e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Desktop\\Projects\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rk871\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfc1cf861594a1aac0598e830ae5714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c8d5bd4c424f399fa7c8f709e41062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3470a00f18044bbbe428cd15238d89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1ad1ba15d041fd83e2e1bc7ce9acac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426690cc535142e09ba80c903d9b19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7a08a7f06e484a98adc572d47d13f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa08b8b564940df9bf7d27d162d2f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6559e0f5795e482387c0d33bf8994ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1f483b9988448cb6dddd994d06ee99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467fc84ebc004209aee15f41e9f10f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5075d1333c8b4528b4b7cb8b01e79b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our embeddings: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load the embedding model\n",
    "# 'all-MiniLM-L6-v2' is a fantastic, fast, and small model.\n",
    "# It runs 100% on your local machine.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Embed all our chunks\n",
    "# This will take a moment as it \"reads\" and \"understands\" each chunk.\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "\n",
    "print(f\"Shape of our embeddings: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93307374",
   "metadata": {},
   "source": [
    "Vector Store with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54727b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 5 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Get the dimension of our vectors (e.g., 384)\n",
    "d = chunk_embeddings.shape[1]\n",
    "\n",
    "# 1. Create a FAISS index\n",
    "# IndexFlatL2 is the simplest, most basic index. It calculates\n",
    "# the exact distance (L2 distance) between our query and all vectors.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# 2. Add our chunk embeddings to the index\n",
    "# We must convert to float32 for FAISS\n",
    "index.add(np.array(chunk_embeddings).astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eca799",
   "metadata": {},
   "source": [
    "Retrieve, Augment, Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "104d9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18de60de4b9942dfb27efee1e5cd6191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebf059b3c994a6882664f14aadb7e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fab36397c04875a7341d88f96e2bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d48898ecf34aac9473162de8a81298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b66d464d77349e1ab9f6437282a5599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae293877d2341599ba05b7760ad379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73e908b3be84f55b2dd3ea4310b0832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoeForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GlmMoeDsaForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3_5ForCausalLM', 'Qwen3_5MoeForCausalLM', 'Qwen3_5MoeForCausalLM', 'Qwen3_5ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'YoutuForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Load a \"Question-Answering\" or \"Text-Generation\" model\n",
    "# We'll use a small, instruction-tuned model from Google.\n",
    "generator = pipeline('text-generation', model='google/flan-t5-small')\n",
    "\n",
    "# --- This is our RAG pipeline function ---\n",
    "def answer_question(query):\n",
    "    # 1. RETRIEVE\n",
    "    # Embed the user's query\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "    # Search the FAISS index for the top k (e.g., k=2) most similar chunks\n",
    "    k = 2\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Get the actual text chunks from our original 'chunks' list\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # 2. AUGMENT\n",
    "    # This is the \"magic prompt.\" We combine the retrieved context\n",
    "    # with the user's query.\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the following question using *only* the provided context.\n",
    "    If the answer is not in the context, say \"I don't have that information.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. GENERATE\n",
    "    # Feed the augmented prompt to our generative model\n",
    "    answer = generator(prompt_template, max_length=100)\n",
    "    print(f\"--- CONTEXT ---\\n{context}\\n\")\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aed340",
   "metadata": {},
   "source": [
    "some questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f4b512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the WFH policy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONTEXT ---\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "Company Policy Manual:\n",
      "\n",
      "Answer: \n",
      "    Answer the following question using *only* the provided context.\n",
      "    If the answer is not in the context, say \"I don't have that information.\"\n",
      "\n",
      "    Context:\n",
      "    - WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "Company Policy Manual:\n",
      "\n",
      "    Question:\n",
      "    What is the WFH policy?\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"What is the WFH policy?\"\n",
    "print(f\"Query: {query_1}\")\n",
    "print(f\"Answer: {answer_question(query_1)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03bfc532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the company's dental plan?\n",
      "--- CONTEXT ---\n",
      "Company Policy Manual:\n",
      "\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "Answer: \n",
      "    Answer the following question using *only* the provided context.\n",
      "    If the answer is not in the context, say \"I don't have that information.\"\n",
      "\n",
      "    Context:\n",
      "    Company Policy Manual:\n",
      "\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "    Question:\n",
      "    What is the company's dental plan?\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"What is the company's dental plan?\"\n",
    "print(f\"Query: {query_2}\")\n",
    "print(f\"Answer: {answer_question(query_2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790d3c7",
   "metadata": {},
   "source": [
    "Final Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864d798",
   "metadata": {},
   "source": [
    "Take a step back. What you just built in a few dozen lines of Python is the foundation of the next generation of AI. You solved the three biggest problems with LLMs:\n",
    "\n",
    "1. Hallucinations: You grounded the model in reality.\n",
    "2. Stale Knowledge: You can update the knowledge! Just re-run the indexing (Steps 1-4) on new documents.\n",
    "3. Data Privacy: No data ever left your computer. The embedding model and the LLM all ran locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
